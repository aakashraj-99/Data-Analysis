{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Initialization***"
      ],
      "metadata": {
        "id": "DrhqpVNnOfqR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpzRWDzJBBG4",
        "outputId": "71ad3ee7-113e-44b4-8916-b915b775bea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "#Install PySpark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import col, avg"
      ],
      "metadata": {
        "id": "q3_2JBkyBEB-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "emCeyX40BTdl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV files into Spark DataFrames\n",
        "df_games = spark.read.csv(\"nflgames.csv\", header=True, inferSchema=True)\n",
        "df_players = spark.read.csv(\"nflplayers.csv\", header=True, inferSchema=True)\n",
        "df_positions = spark.read.csv(\"nflpositions.csv\", header=True, inferSchema=True)\n",
        "df_teams = spark.read.csv(\"nflteams.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "tkPtXYNDBWtb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code performs several tasks related to setting up a PySpark environment, initializing a Spark session, and loading CSV files into Spark DataFrames. Here's a description of each part of the code:\n",
        "\n",
        "1. Install PySpark: This line installs the PySpark library using the pip package manager. PySpark is a Python library for Apache Spark, a distributed data processing framework.\n",
        "\n",
        "2. Import necessary libraries: This section imports the required libraries for working with PySpark and data manipulation. It includes:\n",
        "   - `SparkSession` from `pyspark.sql`: This is used to create and manage a Spark session.\n",
        "   - `pandas` as `pd`: Pandas is a popular library for data manipulation and can be used alongside PySpark.\n",
        "   - `col` and `avg` from `pyspark.sql.functions`: These functions are used for column operations and calculating the average, respectively.\n",
        "\n",
        "3. Initialize Spark session: This code initializes a Spark session using the `SparkSession.builder.master(\"local[*]\").getOrCreate()` method. It creates a Spark session with the specified configuration. In this case, `\"local[*]\"` means Spark will run in local mode using all available CPU cores.\n",
        "\n",
        "4. Load CSV files into Spark DataFrames: This section reads CSV files (`nflgames.csv`, `nflplayers.csv`, `nflpositions.csv`, and `nflteams.csv`) into Spark DataFrames. The `spark.read.csv` method is used for this purpose. The `header=True` argument indicates that the first row of each CSV file contains column headers, and `inferSchema=True` attempts to automatically infer the data types of the columns.\n",
        "\n",
        "Overall, this code prepares the environment for data analysis using PySpark by setting up a Spark session and loading the necessary data into DataFrames for further processing."
      ],
      "metadata": {
        "id": "xoAmEieEQG7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SUMMARY OF THE DATASET***"
      ],
      "metadata": {
        "id": "ZOCRGsCRMCic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display schema and sample data\n",
        "df_games.printSchema()\n",
        "df_games.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM9cv8FmBwOr",
        "outputId": "261be642-7f42-4cc6-9eb1-c749f5f015af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- GameID: integer (nullable = true)\n",
            " |-- Week: integer (nullable = true)\n",
            " |-- HomeTeamID: integer (nullable = true)\n",
            " |-- AwayTeamID: integer (nullable = true)\n",
            " |-- HomeScore: integer (nullable = true)\n",
            " |-- AwayScore: integer (nullable = true)\n",
            " |-- DayOfWeek: string (nullable = true)\n",
            " |-- TimeOfDay: string (nullable = true)\n",
            " |-- FieldType: string (nullable = true)\n",
            " |-- Temp: integer (nullable = true)\n",
            " |-- Wind: integer (nullable = true)\n",
            "\n",
            "+------+----+----------+----------+---------+---------+---------+---------+----------+----+----+\n",
            "|GameID|Week|HomeTeamID|AwayTeamID|HomeScore|AwayScore|DayOfWeek|TimeOfDay| FieldType|Temp|Wind|\n",
            "+------+----+----------+----------+---------+---------+---------+---------+----------+----+----+\n",
            "|     1|   1|         1|        29|       16|       23|      Sun|      Day| sportturf|  74|   8|\n",
            "|     2|   2|         1|        28|       26|        6|      Thu|    Night| sportturf|  82|   6|\n",
            "|     3|   3|        27|         1|       21|       23|      Sun|      Day|    grass |  71|  23|\n",
            "|     4|   4|         1|        15|       38|       10|      Sun|      Day| sportturf|  78|   4|\n",
            "|     5|   5|         7|         1|       20|       13|      Sun|      Day|fieldturf |   0|   0|\n",
            "+------+----+----------+----------+---------+---------+---------+---------+----------+----+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_positions.printSchema()\n",
        "df_positions.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv_VB7z2CPGl",
        "outputId": "a01e35db-a2c1-486e-cb27-457981ef6fe5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- PositionID: integer (nullable = true)\n",
            " |-- PositionName: string (nullable = true)\n",
            "\n",
            "+----------+------------+\n",
            "|PositionID|PositionName|\n",
            "+----------+------------+\n",
            "|         1|          QB|\n",
            "|         2|          RB|\n",
            "|         3|          WR|\n",
            "|         4|          TE|\n",
            "|         5|           K|\n",
            "+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_players.printSchema()\n",
        "df_players.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoVSt_KpCVuN",
        "outputId": "3d2b700d-6b60-4656-d185-6986fc568355"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- PlayerID: integer (nullable = true)\n",
            " |-- PlayerName: string (nullable = true)\n",
            " |-- PositionID: integer (nullable = true)\n",
            " |-- TeamID: integer (nullable = true)\n",
            "\n",
            "+--------+-----------------+----------+------+\n",
            "|PlayerID|       PlayerName|PositionID|TeamID|\n",
            "+--------+-----------------+----------+------+\n",
            "|       1|      Kamar Aiken|         3|     1|\n",
            "|       2|     Marlon Brown|         3|     1|\n",
            "|       3|    Jeremy Butler|         3|     1|\n",
            "|       4|Michael Campanaro|         3|     1|\n",
            "|       5|     Owen Daniels|         4|     1|\n",
            "+--------+-----------------+----------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_teams.printSchema()\n",
        "df_teams.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_SMWh6MCZk6",
        "outputId": "1b8aa1f1-ff45-47b1-e527-ec100132c134"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- TeamID: integer (nullable = true)\n",
            " |-- TeamName: string (nullable = true)\n",
            " |-- TeamCapsAbrv: string (nullable = true)\n",
            " |-- TeamAbrv: string (nullable = true)\n",
            "\n",
            "+------+-------------------+------------+--------+\n",
            "|TeamID|           TeamName|TeamCapsAbrv|TeamAbrv|\n",
            "+------+-------------------+------------+--------+\n",
            "|     1|   Baltimore Ravens|         RAV|     rav|\n",
            "|     2|     Denver Broncos|         DEN|     den|\n",
            "|     3|    Oakland Raiders|         RAI|     rai|\n",
            "|     4|Philadelphia Eagles|         PHI|     phi|\n",
            "|     5|     Dallas Cowboys|         DAL|     dal|\n",
            "+------+-------------------+------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code displays the schema and a sample of the data from four Spark DataFrames: `df_games`, `df_positions`, `df_players`, and `df_teams`. It helps users understand the structure and contents of these DataFrames, which contain data related to NFL games, player positions, players, and teams."
      ],
      "metadata": {
        "id": "R6LvLJ1vSGyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ***Team that has the highest average AwayScore as the Away Team***"
      ],
      "metadata": {
        "id": "dgMmBOxQMOcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"NFL_HighestAvgAwayScore\").getOrCreate()"
      ],
      "metadata": {
        "id": "YfwxBmXLCqlk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average AwayScore for each Away Team\n",
        "avg_away_scores = df_games.groupBy(\"AwayTeamID\").agg(avg(\"AwayScore\").alias(\"AvgAwayScore\"))\n",
        "\n",
        "# Join df_teams to get the team names for Away Teams\n",
        "avg_away_scores_with_names = avg_away_scores.join(df_teams, avg_away_scores[\"AwayTeamID\"] == df_teams[\"TeamID\"], \"inner\")\n",
        "\n",
        "# Find the team with the highest average AwayScore\n",
        "highest_avg_away_team = avg_away_scores_with_names.orderBy(\"AvgAwayScore\", ascending=False).first()"
      ],
      "metadata": {
        "id": "yy57yMv0DmcV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the result\n",
        "print(\"Team with the Highest Average AwayScore as the Away Team:\")\n",
        "print(\"Team Name:\", highest_avg_away_team[\"TeamName\"])\n",
        "print(\"Average AwayScore:\", highest_avg_away_team[\"AvgAwayScore\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovZHlOFoDmfI",
        "outputId": "d4eb7ee5-e168-4896-e94a-71271ee9f8ee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Team with the Highest Average AwayScore as the Away Team:\n",
            "Team Name: Indianapolis Colts\n",
            "Average AwayScore: 35.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code calculates and identifies the NFL team with the highest average \"AwayScore\" when playing as the away team in games.\n",
        "\n",
        "1. Initializes a Spark session with the name \"NFL_HighestAvgAwayScore.\"\n",
        "2. Groups the `df_games` DataFrame by \"AwayTeamID\" and calculates the average of the \"AwayScore\" for each away team. The result is stored in the DataFrame `avg_away_scores` with the column name \"AvgAwayScore.\"\n",
        "3. Joins the `avg_away_scores` DataFrame with the `df_teams` DataFrame using the \"AwayTeamID\" as the joining key. This step associates team names with the calculated average away scores, creating a new DataFrame called `avg_away_scores_with_names`.\n",
        "4. Orders the `avg_away_scores_with_names` DataFrame in descending order based on the \"AvgAwayScore\" column, effectively sorting teams by their average away scores.\n",
        "5. Retrieves the team with the highest average away score by using `.first()` to get the top row of the sorted DataFrame. The result is stored in the `highest_avg_away_team` variable.\n",
        "6. Prints the team with the highest average away score along with their team name and the average away score."
      ],
      "metadata": {
        "id": "I1SExak-TCF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Highest and Lowest Average Total Score***"
      ],
      "metadata": {
        "id": "yuEf400uNqWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total scores (HomeScore + AwayScore) for each game\n",
        "df_games_with_totals = df_games.withColumn(\"TotalScore\", col(\"HomeScore\") + col(\"AwayScore\"))\n",
        "\n",
        "# Group the data by FieldType and calculate the average total score\n",
        "avg_total_scores_by_fieldtype = df_games_with_totals.groupBy(\"FieldType\").agg(avg(\"TotalScore\").alias(\"AvgTotalScore\"))\n",
        "\n",
        "# Find the FieldType with the highest average total score\n",
        "highest_avg_total_score = avg_total_scores_by_fieldtype.orderBy(\"AvgTotalScore\", ascending=False).first()\n",
        "\n",
        "# Find the FieldType with the lowest average total score\n",
        "lowest_avg_total_score = avg_total_scores_by_fieldtype.orderBy(\"AvgTotalScore\").first()"
      ],
      "metadata": {
        "id": "Jxqa-ajQDmim"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the results\n",
        "print(\"FieldType with the Highest Average Total Score:\")\n",
        "print(\"FieldType:\", highest_avg_total_score[\"FieldType\"])\n",
        "print(\"Average Total Score:\", highest_avg_total_score[\"AvgTotalScore\"])\n",
        "\n",
        "print(\"\\nFieldType with the Lowest Average Total Score:\")\n",
        "print(\"FieldType:\", lowest_avg_total_score[\"FieldType\"])\n",
        "print(\"Average Total Score:\", lowest_avg_total_score[\"AvgTotalScore\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCW2lVaCDmqB",
        "outputId": "4782d679-ec30-4dcb-f424-c183b27dbc31"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FieldType with the Highest Average Total Score:\n",
            "FieldType: astroplay\n",
            "Average Total Score: 48.0\n",
            "\n",
            "FieldType with the Lowest Average Total Score:\n",
            "FieldType: grass\n",
            "Average Total Score: 34.833333333333336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code calculates and identifies the NFL game field type (FieldType) with the highest and lowest average total scores (HomeScore + AwayScore). Here's a description of what the code does:\n",
        "\n",
        "1. Adds a new column to the `df_games` DataFrame called \"TotalScore,\" which represents the total points scored in each game. This is computed by summing the \"HomeScore\" and \"AwayScore\" columns.\n",
        "2. Groups the `df_games_with_totals` DataFrame by the \"FieldType\" column and calculates the average total score for each field type. The results are stored in a new DataFrame called `avg_total_scores_by_fieldtype`, with the calculated average total score labeled as \"AvgTotalScore.\"\n",
        "3. Orders the `avg_total_scores_by_fieldtype` DataFrame in descending order based on the \"AvgTotalScore\" column to find the field type with the highest average total score. The result is stored in the `highest_avg_total_score` variable.\n",
        "4. Orders the `avg_total_scores_by_fieldtype` DataFrame in ascending order based on the \"AvgTotalScore\" column to find the field type with the lowest average total score. The result is stored in the `lowest_avg_total_score` variable.\n",
        "5. Displays the results, including the field type with the highest and lowest average total scores, along with their respective average total scores.\n"
      ],
      "metadata": {
        "id": "Uu9395kFTun4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Average Players Per Position***"
      ],
      "metadata": {
        "id": "omniJqnnOGrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average number of players in each position for each team\n",
        "average_players_per_position = df_players.groupBy(\"TeamID\", \"PositionID\").count() \\\n",
        "    .groupBy(\"TeamID\").pivot(\"PositionID\").agg(avg(\"count\"))"
      ],
      "metadata": {
        "id": "XZde1W-_Fj26"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the result\n",
        "average_players_per_position.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LIXe-XxFj6a",
        "outputId": "d09e6714-e356-402c-c2ce-42491640a037"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+---+----+---+----+\n",
            "|TeamID|  1|  2|   3|  4|   5|\n",
            "+------+---+---+----+---+----+\n",
            "|    31|3.0|3.0| 7.0|4.0| 1.0|\n",
            "|    28|3.0|2.0| 7.0|3.0| 1.0|\n",
            "|    27|2.0|3.0| 7.0|4.0| 1.0|\n",
            "|    26|3.0|5.0| 8.0|4.0| 2.0|\n",
            "|    12|3.0|5.0| 8.0|4.0| 1.0|\n",
            "|    22|2.0|4.0| 8.0|3.0| 1.0|\n",
            "|     1|2.0|3.0| 7.0|3.0| 1.0|\n",
            "|    13|4.0|3.0| 7.0|4.0| 1.0|\n",
            "|    16|3.0|4.0| 9.0|5.0| 1.0|\n",
            "|     6|2.0|4.0|11.0|5.0| 1.0|\n",
            "|     3|3.0|6.0| 7.0|4.0| 1.0|\n",
            "|    20|3.0|4.0| 5.0|2.0| 2.0|\n",
            "|     5|3.0|3.0| 5.0|4.0| 1.0|\n",
            "|    19|2.0|4.0| 5.0|5.0| 1.0|\n",
            "|    15|3.0|4.0| 5.0|4.0|NULL|\n",
            "|    17|4.0|6.0| 5.0|6.0| 1.0|\n",
            "|     9|2.0|6.0| 7.0|4.0| 1.0|\n",
            "|     4|4.0|3.0| 5.0|4.0| 1.0|\n",
            "|     8|3.0|3.0| 9.0|2.0| 1.0|\n",
            "|    23|2.0|6.0| 7.0|6.0| 1.0|\n",
            "+------+---+---+----+---+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code calculates the average number of players in each position for each NFL team and displays the results.\n",
        "\n",
        "1. It first groups the data by two columns: \"TeamID\" and \"PositionID.\" This groups players by their respective teams and positions.\n",
        "\n",
        "2. Then, it counts the number of players in each combination of team and position.\n",
        "\n",
        "3. After counting the players in each position for each team, the code further groups the data by \"TeamID.\" This step prepares the data for a pivot operation.\n",
        "\n",
        "4. The `pivot` function is applied to the grouped data. It essentially transforms the rows into columns, with each unique position ID becoming a separate column. The pivot operation calculates the average count of players for each position within each team. This results in a DataFrame where rows represent teams, and columns represent different positions (e.g., QB, RB, WR, etc.), with the average player count in each position.\n",
        "\n",
        "5. Finally, the code displays the calculated average player counts for each position within each NFL team using the `show()` function.\n"
      ],
      "metadata": {
        "id": "45slGPkPUTJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Stopping the Spark Session***"
      ],
      "metadata": {
        "id": "R8kJdS1SOX6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "VeFK90qUOVdv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When spark.stop() is executed, it shuts down the Spark application gracefully. This means that any ongoing Spark jobs or tasks will be halted, and Spark resources will be released."
      ],
      "metadata": {
        "id": "qMrpeuYDU1FT"
      }
    }
  ]
}